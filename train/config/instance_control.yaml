flux_path: "black-forest-labs/FLUX.1-dev"
#flux_path: "black-forest-labs/FLUX.1-schnell"
dtype: "bfloat16"

run_name: "before_debug"

model:
  independent_condition: false

train:
  batch_size: 1
  accumulate_grad_batches: 8
  dataloader_workers: 5
  save_interval: 1000
  sample_interval: 100
  max_steps: -1
  gradient_checkpointing: true # (Turn off for faster training)
  save_path: "runs"
  max_sequence_length: 512

  # Specify the type of condition to use. 
  condition_type: "subject"
  dataset:

    img_root: "/home/xinrui/projects/data/ti_pcb/images_top"
    csv_root: "/home/xinrui/projects/data/ti_pcb/ann_csv_top"

    image_size: 1024
    use_patches: true
    patch_size: 256
    save_debug: false
    max_boxes_per_data: 50

  wandb:
    project: "OminiControl"

  lora_config:
    r: 16
    lora_alpha: 16
    init_lora_weights: "gaussian"
    target_modules: "(.*x_embedder|.*(?<!single_)transformer_blocks\\.[0-9]+\\.norm1\\.linear|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_k|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_q|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_v|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_out\\.0|.*(?<!single_)transformer_blocks\\.[0-9]+\\.ff\\.net\\.2|.*single_transformer_blocks\\.[0-9]+\\.norm\\.linear|.*single_transformer_blocks\\.[0-9]+\\.proj_mlp|.*single_transformer_blocks\\.[0-9]+\\.proj_out|.*single_transformer_blocks\\.[0-9]+\\.attn.to_k|.*single_transformer_blocks\\.[0-9]+\\.attn.to_q|.*single_transformer_blocks\\.[0-9]+\\.attn.to_v|.*single_transformer_blocks\\.[0-9]+\\.attn.to_out)"
    # (Uncomment the following lines to train less parameters while keeping the similar performance)
    # target_modules: "(.*(?<!single_)transformer_blocks\\.[0-9]+\\.norm1\\.linear|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_k|.*(?<!single_)transformer_blocks\\.[0-9]+\\.attn\\.to_q|.*single_transformer_blocks\\.[0-9]+\\.norm\\.linear|.*single_transformer_blocks\\.[0-9]+\\.attn.to_k|.*single_transformer_blocks\\.[0-9]+\\.attn.to_q)"

  optimizer:
    type: "Prodigy"
    params:
      lr: 1
      use_bias_correction: true
      safeguard_warmup: true
      weight_decay: 0.01

  # (To use AdamW Optimizer, uncomment the following lines)
  # optimizer:
  #   type: AdamW
  #   lr: 1e-4
  #   weight_decay: 0.001